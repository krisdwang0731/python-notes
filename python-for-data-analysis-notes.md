# Python for Data Analysis 相关笔记
## 统计学知识
### 协方差 （Covariance）
可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。
> 百度百科定义：协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。

期望值分别为E[X]与E[Y]的两个实随机变量X与Y之间的协方差Cov(X,Y)定义为：
$$Cov(X,Y) = E[(X - \mu_x)(Y - \mu_y)] $$
公式的简单解释：如果有两个变量X，Y，每个时刻的“X与其均值之差”乘以“Y与其均值之差”得到一个乘积，再对这每时刻的乘积求和并求出均值（其实是求“期望”）、

>参考链接 [如何通俗易懂地解释「协方差」与「相关系数」的概念](https://www.zhihu.com/question/20852004)

### 标准差 （Standard Deviation）
标准差（Standard Deviation） ，中文环境中又常称均方差，是离均差平方的算术平均数的平方根，用σ表示。在概率统计中最常使用作为统计分布程度上的测量。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。
> 参考链接 [标准差](https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E5%B7%AE)

$$
\sigma= \sqrt{\frac{1}{N}\sum_{i=1}^{N}{(x_i - \mu)}^2}
$$

> 其实标准差就是方差的算术平方根

###相关系数  
一般情况下，相关系数的公式为：
$$\rho = \frac{Cov(X,Y)} {\sigma_x \sigma_y}$$
就是用X、Y的协方差除以X的标准差和Y的标准差。相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差。既然是一种特殊的协方差，那它的意义如下：、

* 也可以反映两个变量变化时是同向还是反向，如果同向变化就为正，反向变化就为负
* 由于它是标准化后的协方差，因此其重要性就体现出来了：它消除了两个变量变化幅度的影响，而只是单纯地反映两个变量每单位变化时的相似程度。
>参考链接 [如何通俗易懂地解释「协方差」与「相关系数」的概念](https://www.zhihu.com/question/20852004)

以上的相关系数是通常所说的**皮尔逊相关系数**,还有其它的几个相关系数，具体参见
[统计学习--三种常见的相关系数](https://zhuanlan.zhihu.com/p/34717666)

* Pearson相关系数是在原始数据的方差和协方差基础上计算得到，所以对离群值比较敏感，它度量的是线性相关。因此，即使pearson相关系数为0，也只能说明变量之间不存在线性相关，但仍有可能存在曲线相关。

* Spearman相关系数和kendall相关系数都是建立在秩和观测值的相对大小的基础上得到，是一种更为一般性的非参数方法，对离群值的敏感度较低，因而也更具有耐受性，度量的主要是变量之间的联系。




